{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download `The movies` dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: permission denied: /Users/anhpham/development/aalto/snlp-22/the-movies-dataset.zip\n"
     ]
    }
   ],
   "source": [
    "! set DATASET_FILE=`$(pwd)/the-movies-dataset.zip`\n",
    "! [ ! -f ${DATASET_FILE} ] && kaggle datasets download -d rounakbanik/the-movies-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: permission denied: /Users/anhpham/development/aalto/snlp-22/dataset\n"
     ]
    }
   ],
   "source": [
    "! set DATASET_DIR=`$(pwd)/dataset`\n",
    "! [ ! -d ${DATASET_DIR} ] && [! -L ${DATASET_DIR}] && mkdir -p ${DATASET_DIR} && unzip ${DATASET_FILE} -d ${DATASET_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataset\n",
      "Size: 32269\n",
      "First 10 rows of corpus:\n",
      "\n",
      "0    [led, woody, andy's, toy, live, happily, room,...\n",
      "1    [sibling, judy, peter, discover, enchanted, bo...\n",
      "2    [family, wedding, reignites, ancient, feud, ne...\n",
      "3    [cheated, mistreated, stepped, woman, holding,...\n",
      "4    [george, bank, ha, recovered, daughter's, wedd...\n",
      "5    [obsessive, master, thief, neil, mccauley, lea...\n",
      "6    [ugly, duckling, undergone, remarkable, change...\n",
      "7    [mischievous, young, boy, tom, sawyer, witness...\n",
      "8    [international, action, superstar, jean, claud...\n",
      "9    [james, bond, must, unmask, mysterious, head, ...\n",
      "Name: corpus, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "movies_by_language = utils.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./dataset/\"\n",
    "\n",
    "df_metadata = pd.read_csv(DATASET_PATH + \"movies_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter columns\n",
    "metadata_cols = [\"id\", \"title\", \"overview\", \"tagline\", \"genres\", \"original_language\", \"poster_path\"]\n",
    "df_metadata   = df_metadata[metadata_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter language\n",
    "in_english     = df_metadata[\"original_language\"] == \"en\"\n",
    "df_metadata_en = df_metadata[in_english]\n",
    "df_metadata_en = df_metadata_en[[col for col in metadata_cols if col != \"original_language\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_metadata_en[\"overview\"]))\n",
    "\n",
    "\n",
    "overviews = []\n",
    "for row in df_metadata_en[\"overview\"]:\n",
    "    overviews.append(str(row))\n",
    "\n",
    "#print(len(overviews))\n",
    "\n",
    "titles = []\n",
    "for row in df_metadata_en[\"title\"]:\n",
    "    titles.append(str(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning / tokenizing finctions:\n",
    "def remove_non_ascii(textstring):\n",
    "    \"\"\"This function removes non-ASCII characters from the text string\"\"\"\n",
    "    return \"\".join(i for i in textstring if ord(i) < 128)\n",
    "\n",
    "\n",
    "def tokenize_and_lowercase(textstring):\n",
    "    \"\"\"This function tokenizes the text string into lowercased tokens with Regex Tokenizer\"\"\"\n",
    "    tokens = []\n",
    "    tokenized_into_sentences = sent_tokenize(textstring)\n",
    "    for e in tokenized_into_sentences:\n",
    "        regex_tokenizer = re.compile(\"\\w+[-'.]\\w+|\\w+\")  # tokenizer removes whitespaces and punctuation\n",
    "        tokenized = re.findall(regex_tokenizer, e)\n",
    "        for e in tokenized:\n",
    "            tokens.append(e.lower())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_stop_words(tokenized_text, stop_words):\n",
    "    \"\"\"This function removes stop words from lowercased tokenized text\"\"\"\n",
    "    clean_text = [x for x in tokenized_text if x not in stop_words]\n",
    "    return clean_text\n",
    "\n",
    "# Cleaning the movie overviews:\n",
    "stop_words = set(stopwords.words(\"english\"))  # setting the list of stopwords to get rid of\n",
    "clean_overviews = []\n",
    "\n",
    "for overview in overviews:\n",
    "    overview_ascii = remove_non_ascii(overview)\n",
    "    overview_tokenized_and_lowercased = tokenize_and_lowercase(overview_ascii)\n",
    "    overview_clean = remove_stop_words(overview_tokenized_and_lowercased, stop_words)\n",
    "    clean_overviews.append(overview_clean)\n",
    "\n",
    "# Examples of output:\n",
    "print(len(clean_overviews))\n",
    "\n",
    "corpus = clean_overviews\n",
    "print(corpus[9])\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jumanji'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = movies_by_language[\"corpus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained word embedding from GoogleNews:\n",
    "\n",
    "embeddings = api.load('word2vec-google-news-300', return_path=True)\n",
    "GoogleNews_word2vec = KeyedVectors.load_word2vec_format(embeddings, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training our corpus with GoogleNews embedding (CBOW architecture)\n",
    "# [source for this piece: https://www.kdnuggets.com/2020/08/content-based-recommendation-system-word-embeddings.html]:\n",
    "\n",
    "GoogleNews_model = Word2Vec(vector_size=300, window=5, min_count=2, workers=-1, sg=0) # sg=0 indicates CBOW architecture\n",
    "GoogleNews_model.build_vocab(corpus)\n",
    "GoogleNews_model.wv.vectors_lockf = np.ones(len(GoogleNews_model.wv), dtype=np.float32)\n",
    "GoogleNews_model.wv.intersect_word2vec_format(embeddings, binary=True)\n",
    "GoogleNews_model.train(corpus, total_examples=GoogleNews_model.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Word2Vec embeddings for each overview (CBOW architecture)\n",
    "# [source for this piece: https://www.kdnuggets.com/2020/08/content-based-recommendation-system-word-embeddings.html]:\n",
    "\n",
    "def vectors(clean_overviews):\n",
    "    global word_embeddings\n",
    "    word_embeddings = []\n",
    "    \n",
    "    for overview in clean_overviews:\n",
    "        word2vec = None\n",
    "        count = 0\n",
    "        for word in overview:\n",
    "          #  print(word)\n",
    "            if word in GoogleNews_model.wv.key_to_index:\n",
    "                count += 1\n",
    "                if word2vec is None:\n",
    "                    word2vec = GoogleNews_model.wv.key_to_index[word]\n",
    "                else:\n",
    "                    word2vec = word2vec + GoogleNews_model.wv.key_to_index[word]\n",
    "                \n",
    "        if word2vec is not None:\n",
    "            word2vec = word2vec / count\n",
    "        \n",
    "        word_embeddings.append(word2vec)\n",
    "    \n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for recommending the top 5 similar movies (CBOW architecture):\n",
    "\n",
    "def recommending(title):\n",
    "    vectors_list=vectors(corpus.tolist())\n",
    "    idx=movies_by_language[movies_by_language[\"title\"] == title].index.tolist()[0]\n",
    "   # print(idx)\n",
    "    length = len(vectors_list)\n",
    "    similarities = []\n",
    "    for i in range(0,length-1):\n",
    "        try:\n",
    "            similarity = cosine_similarity(np.array(vectors_list[idx]).reshape(1, -1), \n",
    "                                           np.array(vectors_list[i]).reshape(1, -1))\n",
    "            print(similarity)\n",
    "            similarities.append(similarity[0][0])\n",
    "        except ValueError:\n",
    "            similarities.append(0)\n",
    "        \n",
    "    similarities_dict={}\n",
    "    for count, value in enumerate(similarities):\n",
    "        similarities_dict[count]=value\n",
    "\n",
    "    print(similarities)\n",
    "        \n",
    "    sorted_similarities_dict = dict(sorted(similarities_dict.items(),\n",
    "                           key=lambda item: item[1],\n",
    "                           reverse=True))\n",
    "    \n",
    "    indices_top=list(sorted_similarities_dict.keys())[1:6]\n",
    "    values_top=list(sorted_similarities_dict.values())\n",
    "    print(\"Top 5 movies most similar to\", title)\n",
    "    print(\"===================================\")\n",
    "    for index in indices_top:\n",
    "        print(movies_by_language[\"title\"][index])\n",
    "        print(\"\\n\")\n",
    "        print(movies_by_language[\"overview\"][index])\n",
    "        print(\"\\n\")\n",
    "        print(\"Accuracy: {:.4f}\".format(values_top[index]))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/anhpham/development/aalto/snlp-22/data.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anhpham/development/aalto/snlp-22/data.ipynb#ch0000020?line=0'>1</a>\u001b[0m \u001b[39m# Testing the recommendations (CBOW architecture):\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anhpham/development/aalto/snlp-22/data.ipynb#ch0000020?line=2'>3</a>\u001b[0m title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGoldenEye\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anhpham/development/aalto/snlp-22/data.ipynb#ch0000020?line=3'>4</a>\u001b[0m recommending(title)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anhpham/development/aalto/snlp-22/data.ipynb#ch0000020?line=5'>6</a>\u001b[0m title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDeathline\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anhpham/development/aalto/snlp-22/data.ipynb#ch0000020?line=6'>7</a>\u001b[0m recommending(title)\n",
      "\u001b[1;32m/Users/anhpham/development/aalto/snlp-22/data.ipynb Cell 20'\u001b[0m in \u001b[0;36mrecommending\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anhpham/development/aalto/snlp-22/data.ipynb#ch0000019?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrecommending\u001b[39m(title):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anhpham/development/aalto/snlp-22/data.ipynb#ch0000019?line=3'>4</a>\u001b[0m     vectors_list\u001b[39m=\u001b[39mvectors(corpus\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anhpham/development/aalto/snlp-22/data.ipynb#ch0000019?line=4'>5</a>\u001b[0m     idx\u001b[39m=\u001b[39mmovies_by_language[movies_by_language[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m title]\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anhpham/development/aalto/snlp-22/data.ipynb#ch0000019?line=5'>6</a>\u001b[0m    \u001b[39m# print(idx)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectors' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing the recommendations (CBOW architecture):\n",
    "\n",
    "title=\"GoldenEye\"\n",
    "recommending(title)\n",
    "\n",
    "title=\"Deathline\"\n",
    "recommending(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our corpus with GoogleNews embedding (Skip-Gram architecture)\n",
    "# [source for this piece: https://www.kdnuggets.com/2020/08/content-based-recommendation-system-word-embeddings.html]:\n",
    "\n",
    "GoogleNews_model_SG = Word2Vec(vector_size=300, window=5, min_count=2, workers=-1, sg=1) # sg=1 indicates Skip-Gram architecture\n",
    "GoogleNews_model_SG.build_vocab(corpus)\n",
    "GoogleNews_model_SG.wv.vectors_lockf = np.ones(len(GoogleNews_model_SG.wv), dtype=np.float32)\n",
    "GoogleNews_model_SG.wv.intersect_word2vec_format(embeddings, lockf=1.0, binary=True)\n",
    "GoogleNews_model_SG.train(corpus, total_examples=GoogleNews_model_SG.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Word2Vec embeddings for each overview (Skip-Gram architecture)\n",
    "# [source for this piece: https://www.kdnuggets.com/2020/08/content-based-recommendation-system-word-embeddings.html]:\n",
    "\n",
    "def vectors_SG(clean_overviews):\n",
    "    global word_embeddings_SG\n",
    "    word_embeddings_SG = []\n",
    "    \n",
    "    for overview in clean_overviews:\n",
    "        word2vec = None\n",
    "        count = 0\n",
    "        for word in overview:\n",
    "          #  print(word)\n",
    "            if word in GoogleNews_model_SG.wv.key_to_index:\n",
    "                count += 1\n",
    "                if word2vec is None:\n",
    "                    word2vec = GoogleNews_model_SG.wv.key_to_index[word]\n",
    "                else:\n",
    "                    word2vec = word2vec + GoogleNews_model_SG.wv.key_to_index[word]\n",
    "                \n",
    "        if word2vec is not None:\n",
    "            word2vec = word2vec / count\n",
    "        \n",
    "        word_embeddings_SG.append(word2vec)\n",
    "    \n",
    "    return word_embeddings_SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for recommending the top 5 similar movies (Skip-Gram architecture):\n",
    "\n",
    "def recommending(title):\n",
    "    vectors_list=vectors_SG(clean_overviews)\n",
    "    idx=titles.index(title)\n",
    "   # print(idx)\n",
    "    length = len(vectors_list)\n",
    "    similarities = []\n",
    "    for i in range(0,length-1):\n",
    "        try:\n",
    "            similarity = cosine_similarity(np.array(vectors_list[idx]).reshape(1, -1), \n",
    "                                           np.array(vectors_list[i]).reshape(1, -1))\n",
    "            similarities.append(similarity[0][0])\n",
    "        except ValueError:\n",
    "            similarities.append(0)\n",
    "        \n",
    "    similarities_dict={}\n",
    "    for count, value in enumerate(similarities):\n",
    "        similarities_dict[count]=value\n",
    "        \n",
    "    sorted_similarities_dict = dict(sorted(similarities_dict.items(),\n",
    "                           key=lambda item: item[1],\n",
    "                           reverse=True))\n",
    "    \n",
    "    indices_top=list(sorted_similarities_dict.keys())[1:6]\n",
    "    print(\"Top 5 movies most similar to\", title)\n",
    "    print(\"===================================\")\n",
    "    for index in indices_top:\n",
    "        print(titles[index])\n",
    "        print(\"\\n\")\n",
    "        print(overviews[index])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the recommendations (Skip-Gram architecture):\n",
    "\n",
    "title=\"Deathline\"\n",
    "recommending(title)\n",
    "\n",
    "\n",
    "# https://image.tmdb.org/t/p/original/7G9915LfUQ2lVfwMEEhDsn3kT4B.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse names from genres column\n",
    "genre_names               = list(map(lambda g: sorted(re.findall(\"'name':\\s*'(\\w*)'\", g)), df_metadata_en[\"genres\"]))\n",
    "genre_names_joined        = list(map(lambda g: \" \".join(g), genre_names))\n",
    "df_metadata_en[\"genres_\"] = genre_names_joined\n",
    "\n",
    "# combine features for tf-idf\n",
    "df_metadata_en[\"document\"] = (\n",
    "    df_metadata_en[\"title\"].astype(str) + \". \" #+ \n",
    "    # df_metadata_en[\"overview\"].astype(str) + \". \" + \n",
    "    # df_metadata_en[\"tagline\"].astype(str) + \". \" + \n",
    "    # df_metadata_en[\"genres_\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "\n",
    "def get_ents(doc):\n",
    "    return [\"_\".join(e.text.upper().split()) for e in nlp(doc).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(df_metadata_en[\"document\"])\n",
    "vocab  = {}\n",
    "\n",
    "# pre-process corpus (named-entity recognition, stopword removal, lemmatization)\n",
    "for i, doc in enumerate(corpus):\n",
    "    # split to tokens and remove stopwords\n",
    "    tokens = [t for t in word_tokenize(doc.lower()) if t not in stop_words]\n",
    "    # lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    # prepend entities\n",
    "    tokens = get_ents(doc) + tokens\n",
    "    # join tokens with whitespace and finish pre-processing\n",
    "    corpus[i] = \" \".join(tokens)\n",
    "    for t in tokens:\n",
    "        if t not in vocab:\n",
    "            vocab[t] = True\n",
    "\n",
    "# generate vocab\n",
    "vocab = list(dict.keys(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# calc tf-idf\n",
    "tf_idf_vec = TfidfVectorizer(lowercase=False, stop_words=None, vocabulary=vocab, smooth_idf=True, use_idf=True)\n",
    "tf_idf     = tf_idf_vec.fit_transform(corpus)\n",
    "\n",
    "titles        = list(df_metadata_en[\"title\"])\n",
    "movie_titles  = [\"Ex Machina\", \"The Shawshank Redemption\", \"Prometheus\", \"The Dark Knight\"]\n",
    "movie_indices = [titles.index(t) for t in movie_titles]\n",
    "\n",
    "# print first doc\n",
    "for i in movie_indices:\n",
    "    df = pd.DataFrame(tf_idf[i].T.todense(), index=tf_idf_vec.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "    x = df.sort_values(\"TF-IDF\", ascending=False).head(3)\n",
    "    print(titles[i])\n",
    "    print(\"=============\")\n",
    "    print(x)\n",
    "    print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix\n",
    "sim = cosine_similarity(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in movie_indices:\n",
    "    top_3_indices = list(sim[i].argsort()[::-1][1:4])\n",
    "    print(f\"Top 3 similar movies to {titles[i]}\")\n",
    "    print(\"====================================\")\n",
    "    for j in top_3_indices:\n",
    "        print(f\"- {titles[j]} (sim: {sim[i,j]})\")\n",
    "    print(\"====================================\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce93d1a4f56540fabf8f1c76c3aa6cf9757ad5e7d65f9c94603a0a68aa2226c4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
